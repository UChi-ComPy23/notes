{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization, Numpy Universal Functions\n",
    "\n",
    "In order to write performant code using numerical libraries, it is useful to keep the following rule in mind:\n",
    "> Code that is predictable can be made fast\n",
    "\n",
    "The most common example of predictable code is a fixed length loop which performs the same operation at each iteration (up to changes in index)\n",
    "\n",
    "Examples\n",
    "1. Matrix-vector multiplication\n",
    "2. Functions applied element-wise to an array\n",
    "\n",
    "Non-examples:\n",
    "1. Code with branch instructions (`if`, `else`, etc.)\n",
    "2. Code with recursive function calls (at least in Python)\n",
    "\n",
    "One reason why predictable code can be fast is that most CPUs have what is called a [branch predictor](https://en.wikipedia.org/wiki/Branch_predictor) in them, which pre-loads computation.  If a branch is predicted incorrectly, then the CPU has to switch gears and go along the correct brach, which takes time.  Code without branches will minimize the number of branch prediction errors, speeding up code.\n",
    "\n",
    "Another reason why predictable code can be made fast is vectorization.  If you are performing the same operation in a predictable way, code can employ special instructions such as [AVX](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) which can greatly increase efficiency.\n",
    "\n",
    "You don't need to worry about the details in Python, but it is good to know how to write code that allows libraries like NumPy to take advantage of these techniques.  Note that standard Python loops will not take advantage of these things - you typically need to use libraries.\n",
    "\n",
    "## Universal Functions\n",
    "\n",
    "Numpy universal functions (or ufuncs) are functions that are applied element-wise to an array.  Examples include most math operations and logical comparisons.  You can find additional information in [the ufunc documentation](https://numpy.org/doc/stable/reference/ufuncs.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up two vectors\n",
    "n = 1_000_000\n",
    "x = np.random.randn(n)\n",
    "y = np.random.randn(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add(x, y):\n",
    "    \"\"\"\n",
    "    add two arrays using a Python for-loop\n",
    "    \"\"\"\n",
    "    z = np.empty_like(x)\n",
    "    for i in range(len(x)):\n",
    "        z[i] = x[i] + y[i]\n",
    "        \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for naive add: 2.965e-01 sec\n",
      "time for numpy add: 4.709e-03 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.monotonic()\n",
    "z = naive_add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for naive add: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "z = np.add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numpy add: {:0.3e} sec\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer to write your code without the for-loop, you can use `np.vectorize`.  See [the documentation](https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for naive add: 2.570e-01 sec\n",
      "time for numpy add: 4.580e-03 sec\n",
      "time for numpy add (vectorize): 2.545e-01 sec\n"
     ]
    }
   ],
   "source": [
    "@np.vectorize\n",
    "def numpy_add_vectorize(x, y):\n",
    "    return x + y\n",
    "\n",
    "start = time.monotonic()\n",
    "z = naive_add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for naive add: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "z = np.add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numpy add: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "z = numpy_add_vectorize(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numpy add (vectorize): {:0.3e} sec\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.vectorize` doesn't really give a performance boost, but can make defining functions simpler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. perform some timing tests that compare a naive python loop implementation with a numpy ufunc.  Try computing `sin`, `exp`, multiplication, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complicated functions\n",
    "\n",
    "Some functions that can be sped up considerably are a bit more complicated than ufuncs.  One example is matrix-vector multiplication.  We'll use the notation `Ax = y`, where\n",
    "\\begin{equation}\n",
    "y_i = \\sum_j A_{i,j} x_j\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matrix and vector for multiplication\n",
    "m, n = 500, 1000\n",
    "A = np.random.randn(m, n)\n",
    "x = np.random.randn(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matvec(A, x):\n",
    "    \"\"\"\n",
    "    naive matrix-vector multiplication implementation\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    y = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            y[i] = y[i] + A[i,j] * x[j]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for naive matvec: 4.784e-01 sec\n",
      "time for numpy matvec: 6.142e-04 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.19019311536421e-13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.monotonic()\n",
    "y1 = naive_matvec(A, x)\n",
    "end = time.monotonic()\n",
    "print(\"time for naive matvec: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "# y2 = np.matmul(A, x)\n",
    "y2 = A @ x\n",
    "end = time.monotonic()\n",
    "print(\"time for numpy matvec: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "np.linalg.norm(y1 - y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba\n",
    "\n",
    "[Numba](https://numba.pydata.org/) is a just in time (JIT) compiler for Python code.  It provides several decorators which make it very easy to get speedups for numerical code in many situations.\n",
    "\n",
    "Just in time compilation is an increasingly popular solution that bridges the gap between interpreted and compiled languages.  Generally:\n",
    "* Interpreted languages (such as Python) simply read code line-by-line and execute as they go along.\n",
    "* Compiled languages (such as C, C++, fortran) compile code into a binary, which can be optimized to run quickly\n",
    "\n",
    "Compilation takes time intially but saves time when you use the binary.  Python libraries such as NumPy and SciPy use compiled libraries under the hood for speed.  Interpreted languages tend to be slower, but are easier to develop in.\n",
    "\n",
    "Just in time compilation will produce a compiled version of a function the first time it is needed.  [Julia](https://julialang.org/) is a relatively new language which uses JIT to produce fast code with less development overhead.\n",
    "\n",
    "One of the things you need to know to compile code is the types used - if you want to use different types (e.g. single *and* double precision versions of a function), you need different compiled versions.  Python usually allows you to not worry too much about type, but this is one reason why you need to know about it anyways in scientific computing.\n",
    "\n",
    "First:\n",
    "```bash\n",
    "$ conda install numba\n",
    "```\n",
    "\n",
    "Let's look at how Numba can be used with our `naive_add` ufunc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5225/852113555.py:3: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit # this is the only thing we do different\n"
     ]
    }
   ],
   "source": [
    "from numba import jit, njit\n",
    "\n",
    "@jit # this is the only thing we do different\n",
    "def numba_add(x, y):\n",
    "    \"\"\"\n",
    "    add two arrays using a Python for-loop\n",
    "    \"\"\"\n",
    "    z = np.empty_like(x)\n",
    "    for i in range(len(x)):\n",
    "        z[i] = x[i] + y[i]\n",
    "        \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for naive add: 2.072e-01 sec\n",
      "time for numpy add: 2.060e-03 sec\n",
      "time for numba add: 5.392e-01 sec\n"
     ]
    }
   ],
   "source": [
    "# set up two vectors\n",
    "n = 1_000_000\n",
    "x = np.random.randn(n)\n",
    "y = np.random.randn(n)\n",
    "\n",
    "start = time.monotonic()\n",
    "z = naive_add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for naive add: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "z = np.add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numpy add: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "z = numba_add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numba add: {:0.3e} sec\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `numba` JIT function runs in about the same time as the naive function.  Let's see what happens when we run the code again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for naive add: 2.158e-01 sec\n",
      "time for numpy add: 4.374e-03 sec\n",
      "time for numba add: 2.573e-03 sec\n"
     ]
    }
   ],
   "source": [
    "# set up two vectors\n",
    "n = 1_000_000\n",
    "x = np.random.randn(n)\n",
    "y = np.random.randn(n)\n",
    "\n",
    "start = time.monotonic()\n",
    "z = naive_add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for naive add: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "z = np.add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numpy add: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "z = numba_add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numba add: {:0.3e} sec\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `numba` function is *much* faster.  This is because the first time the function is called, it must be compiled.  Every subsequent time you call the function, it will run much faster. \n",
    "\n",
    "The take-away is that it is advantageous to use JIT with functions you will use repeatedly, but not necessarily worth the time for functions you will only use once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced numba\n",
    "\n",
    "You can get a lot of mileage out of `numba` without too much trouble.  It is always good to look at [the documentation](https://numba.readthedocs.io/en/stable/index.html) to learn more.  Here are a few examples:\n",
    "\n",
    "[Parallelization](https://numba.readthedocs.io/en/stable/user/parallel.html#numba-parallel) (this is supported on a handful of known operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import prange # parallel range\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def numba_add_parallel(x, y):\n",
    "    \"\"\"\n",
    "    add two arrays using a Python for-loop\n",
    "    \"\"\"\n",
    "    z = np.empty_like(x)\n",
    "    for i in prange(len(x)):\n",
    "        z[i] = x[i] + y[i]\n",
    "        \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for numba add: 1.805e-01 sec\n",
      "time for numpy add: 1.962e-01 sec\n",
      "time for numba parallel add: 2.842e-01 sec\n"
     ]
    }
   ],
   "source": [
    "# set up two vectors\n",
    "n = 100_000_000\n",
    "x = np.random.randn(n)\n",
    "y = np.random.randn(n)\n",
    "\n",
    "z = numba_add_parallel(x, y) # precompile\n",
    "\n",
    "start = time.monotonic()\n",
    "z = numba_add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numba add: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "z = np.add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numpy add: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "z = numba_add_parallel(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numba parallel add: {:0.3e} sec\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelization of `matvec`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def numba_sumvec(x):\n",
    "    \"\"\"\n",
    "    naive matrix-vector multiplication implementation\n",
    "    \"\"\"\n",
    "    m = x.shape\n",
    "    y = 0\n",
    "    for i in prange(m):\n",
    "        y = y + x[j]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def numba_matvec(A, x):\n",
    "    \"\"\"\n",
    "    naive matrix-vector multiplication implementation\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    y = np.zeros(m)\n",
    "    for i in prange(m):\n",
    "        for j in range(n):\n",
    "            y[i] = y[i] + A[i,j] * x[j]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for numba parallel matvec: 8.006e-03 sec\n",
      "time for numpy matvec: 1.781e-02 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.518584735988475e-12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up matrix and vector for multiplication\n",
    "m, n = 4000, 4000\n",
    "A = np.random.randn(m, n)\n",
    "x = np.random.randn(n)\n",
    "\n",
    "y = numba_matvec(A, x) # precompile\n",
    "\n",
    "start = time.monotonic()\n",
    "y1 = numba_matvec(A, x)\n",
    "end = time.monotonic()\n",
    "print(\"time for numba parallel matvec: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "y2 = np.matmul(A, x)\n",
    "end = time.monotonic()\n",
    "print(\"time for numpy matvec: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "np.linalg.norm(y1 - y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declaring ufuncs without a loop.  You can define a function on elements using `numba.vectorize`.  Unlike `numpy.vectorize`, `numba` will give you a noticeable speedup.  We need to provide a call signature - for example `float32(float32, float32)` will take two different single precision floating point numbers (`float32`) as input, and return a single precision floating point number as output.  See the [documentation](https://numba.pydata.org/numba-doc/dev/user/vectorize.html#vectorize) for additional details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32, float32)']) # call signature defined\n",
    "def numba_add_vectorize(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for numba add: 3.819e-01 sec\n",
      "time for numba add (vectorized): 1.390e-01 sec\n"
     ]
    }
   ],
   "source": [
    "# Initialize arrays\n",
    "n = 100_000_000\n",
    "x = np.ones(n, dtype=np.float32)\n",
    "y = np.ones(x.shape, dtype=x.dtype)\n",
    "z = np.empty_like(x, dtype=x.dtype)\n",
    "\n",
    "# precompile\n",
    "z = numba_add_vectorize(x, y)\n",
    "\n",
    "start = time.monotonic()\n",
    "z = numba_add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numba add: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "z = numba_add_vectorize(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numba add (vectorized): {:0.3e} sec\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For NVIDIA GPUS, you can use cuda through `numba` - see the [Nvidia page](https://developer.nvidia.com/blog/numba-python-cuda-acceleration/) for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def numba_add_cuda(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for numba add: 3.125e-01 sec\n",
      "time for numba add (vectorized): 2.114e-01 sec\n",
      "time for numba add (cuda): 5.494e-01 sec\n"
     ]
    }
   ],
   "source": [
    "# precompile\n",
    "z = numba_add_cuda(x, y)\n",
    "\n",
    "start = time.monotonic()\n",
    "z = numba_add(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numba add: {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "z = numba_add_vectorize(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numba add (vectorized): {:0.3e} sec\".format(end - start))\n",
    "\n",
    "start = time.monotonic()\n",
    "z = numba_add_cuda(x, y)\n",
    "end = time.monotonic()\n",
    "print(\"time for numba add (cuda): {:0.3e} sec\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there may be a bit of a speedup using GPU - this will depend on the type of operation being performed, and if the GPU will be better suited for the computation compared to a vectorized instruction on a CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, see [performance hints](https://numba.readthedocs.io/en/stable/user/performance-tips.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pycourse_test)",
   "language": "python",
   "name": "pycourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
